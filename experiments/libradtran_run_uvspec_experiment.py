#!/usr/bin/env python
"""Run uvspec for a given experimental setup defined in a libradtran_write_input_file_[experiment].py file

Given the experiment name, campaign, the flight and the path to the uvspec executable, this script calls ``uvspec`` for each input file and writes a log and output file.
It does so in parallel, checking how many CPUs are available.
After that the output files are merged into one data frame and information from the input file is added to write one netCDF file.

The script can be run for one flight or for all flights.

**Required User Input:**

* experiment, name of the experiment defining the input path with all input files generated by the corresponding :py:mod:`libradtran_write_input_file_[experiment].py` script and the netCDF file name
* campaign
* flight_key (e.g. "RF17")
* path to uvspec executable

**Output:**

* out and log file for each simulation
* log file for each flight
* netCDF file with simulation in- and output

*author*: Johannes RÃ¶ttenbacher
"""
if __name__ == "__main__":
    # %% module import
    import pylim.helpers as h
    import pylim.halo_ac3 as meta
    from pylim import libradtran
    import numpy as np
    import pandas as pd
    import xarray as xr
    import os
    from operator import itemgetter
    from subprocess import Popen
    from tqdm import tqdm
    from joblib import cpu_count
    import datetime as dt
    from pysolar.solar import get_azimuth

    # %% set options
    experiment = "seaice_thermal"  # string defining experiment name, will be used for input path and netCDF filename
    campaign = "halo-ac3"
    # get all flights from dictionary
    all_flights = [key for key in meta.transfer_calibs.keys()] if campaign == "cirrus-hl" else list(
        meta.flight_names.values())
    all_flights = all_flights[18:19]  # select specific flight[s] if needed

    uvspec_exe = "/opt/libradtran/2.0.4/bin/uvspec"
    solar_flag = False
    # set base paths
    libradtran_base_path = h.get_path("libradtran_exp", campaign=campaign)

    # %% run for all flights
    for flight in all_flights:
        flight_key = flight[-4:] if campaign == "halo-ac3" else flight
        libradtran_path = os.path.join(libradtran_base_path, "wkdir", flight_key, f"{experiment}")
        date = flight[9:17] if campaign == "halo-ac3" else flight[7:15]
        # check if outfile exists already and give a warning
        nc_filepath = f"{libradtran_base_path}/{campaign.swapcase()}_HALO_libRadtran_simulation_{experiment}_{date}_{flight_key}.nc"
        if os.path.isfile(nc_filepath):
            answer = input(f"{nc_filepath}\nalready exists!\n Do you want to overwrite it? (y/n)")
            if answer == "y":
                pass
            else:
                continue
        # get files
        input_files = [os.path.join(libradtran_path, f) for f in os.listdir(libradtran_path) if
                       f.endswith(".inp") and f.startswith(date)]
        input_files.sort()  # sort input files -> output files will be sorted as well

        # %% setup logging
        try:
            file = __file__
        except NameError:
            file = None
        log = h.setup_logging("./logs", file, flight_key)
        log.info(f"Options Given:\n"
                 f"campaign: {campaign}\n"
                 f"flight: {flight}\n"
                 f"experiment: {experiment}\n"
                 f"solar_flag: {solar_flag}\n"
                 f"uvspec_exe: {uvspec_exe}\n"
                 f"Script started: {dt.datetime.utcnow():%c UTC}\n"
                 f"wkdir: {libradtran_path}")

        # %% call uvspec for all files
        try:
            libradtran.run_uvspec_parallel(input_files, uvspec_exe)
        except UserWarning as e:
            log.info(f"{e}\nMoving to next flight")
            continue

        # %% merge output files and write a netCDF file
        latitudes, longitudes, time_stamps, saa, exp_settings = list(), list(), list(), list(), list()

        log.info("Reading input files and extracting information from it...")
        for infile in tqdm(input_files, desc="Input files"):
            input_info = libradtran.get_info_from_libradtran_input(infile)
            lat, lon, ts, header, wavelengths, integrate_flag, zout = itemgetter("latitude", "longitude", "time_stamp",
                                                                                 "header", "wavelengths",
                                                                                 "integrate_flag", "zout")(input_info)
            latitudes.append(lat)
            longitudes.append(lon)
            time_stamps.append(ts)
            # convert timestamp to datetime object with timezone information
            dt_ts = ts.to_pydatetime().astimezone(dt.timezone.utc)
            saa.append(get_azimuth(lat, lon, dt_ts))  # calculate solar azimuth angle
            exp_settings.append(input_info["experiment_settings"])

        log.info("Merging all output files and adding information from input files...")
        output_files = [f.replace(".inp", ".out") for f in input_files]  # generate/get output filenames
        output = pd.concat([pd.read_csv(file, header=None, names=header, sep="\s+").assign(time=ts)
                            for file, ts in zip(tqdm(output_files, desc="Output files"), time_stamps)])
        # convert output altitude to m
        output["zout"] = output["zout"] * 1000
        len_zout = len(input_info["zout"])

        if "lambda" in header and len_zout > 1 and exp_settings[0] is not None:
            # here a spectral simulation with outputs on multiple levels and different experimental setups has been
            # performed
            nr_wavelengths = len(output["lambda"].unique())  # retrieve the number of wavelengths which were simulated
            len_experiment = nr_wavelengths * len_zout
            experiment_settings = {k: list() for k in exp_settings[0].keys()}
            for d in exp_settings:
                for key, value in d.items():
                    experiment_settings[key].append(np.repeat(value, len_experiment))

            # add numeric experiment settings to dataframe and extract additional dimensions for adding to index
            additional_dimensions = list()
            for key in experiment_settings:
                try:
                    output[key] = pd.to_numeric(np.array(experiment_settings[key]).flatten())
                    additional_dimensions.append(key)
                except ValueError as e:
                    log.debug(f"{e}\n"
                              f"Variable '{key}' with value: '{np.unique(experiment_settings[key])}' not added to"
                              f" dataframe.")

        elif "lambda" in header and len_zout > 1:
            # here a spectral simulation with outputs on multiple levels has been performed
            nr_wavelengths = len(output["lambda"].unique())  # retrieve the number of wavelengths which were simulated

        elif "lambda" in header and len_zout == 1:
            # here a spectral simulation for one altitude has been performed resulting in more than one line per file
            nr_wavelengths = len(output["lambda"].unique())  # retrieve the number of wavelengths which were simulated
            # retrieve wavelength independent variables from files
            # since the data frames are all concatenated with their original index we can use only the rows with index 0
            zout = output.loc[0, "zout"]
            sza = output.loc[0, "sza"]

        if len_zout == 1:
            output = output.set_index(["time"]) if integrate_flag else output.set_index(["time", "lambda"])
        elif exp_settings[0] is None:
            output = output.set_index(["time", "zout"]) if integrate_flag else output.set_index(
                ["time", "lambda", "zout"])
        else:
            if integrate_flag:
                output = output.set_index(["time", "zout", additional_dimensions[0], additional_dimensions[1]])
            else:
                output = output.set_index(["time", "lambda", "zout", additional_dimensions[0], additional_dimensions[1]])

        # calculate direct fraction
        output["direct_fraction"] = output["edir"] / (output["edir"] + output["edn"])
        if solar_flag:
            # convert mW/m2 to W/m2 (see page 43 of manual)
            output["edir"] = output["edir"] / 1000
            output["eup"] = output["eup"] / 1000
            output["edn"] = output["edn"] / 1000
            # calculate solar clear sky downward irradiance
            if "eglo" in output:
                output["eglo"] = output["eglo"] / 1000
            else:
                output["eglo"] = output["edir"] + output["edn"]

        # set up some metadata
        integrate_str = "integrated " if integrate_flag else ""
        wavelength_str = f"wavelength range {wavelengths[0]} - {wavelengths[1]} nm"

        # get meta data dictionaries
        global_attributes = libradtran.get_netcdf_global_attributes(campaign, flight, experiment)
        variable_attributes = libradtran.get_netcdf_variable_attributes(solar_flag, integrate_str, wavelength_str)

        encoding = dict(time=dict(units="seconds since 2017-01-01"))

        ds = output.to_xarray()  # convert data frame to data set

        wavelength_independent_variables = ["p", "T", "CLWD", "CIWD"]
        # drop wavelength dimension from wavelength independent variables by either overwriting them or dropping the dim
        if not integrate_flag and len_zout == 1:
            ds = ds.assign(zout=xr.DataArray(zout, coords={"time": ds.time}))
            ds = ds.assign(sza=xr.DataArray(sza, coords={"time": ds.time}))
        elif not integrate_flag and len_zout > 1:
            dims = ds.sza.dims
            sza = ds.sza
            for i, dim in enumerate(dims):
                if dim != "time":
                    sza = sza.isel({f"{dim}": 0}, drop=True)
            ds["sza"] = sza
            for var in wavelength_independent_variables:
                if var in ds:
                    ds[var] = ds[var].isel({"lambda": 0}, drop=True)

        # add the time dependent variables from the input files
        ds = ds.assign(saa=xr.DataArray(np.unique(saa), coords={"time": ds.time}))
        ds = ds.assign(latitude=xr.DataArray(np.unique(latitudes), coords={"time": ds.time}))
        ds = ds.assign(longitude=xr.DataArray(np.unique(longitudes), coords={"time": ds.time}))

        ds = ds.rename({"zout": "altitude"})
        if "lambda" in header:
            ds = ds.rename({"lambda": "wavelength"})

        ds.attrs = global_attributes  # assign global attributes
        # set attributes of each variable
        for var in ds:
            ds[var].attrs = variable_attributes[var]
            encoding[var] = dict(_FillValue=None)  # remove the default _FillValue attribute from each variable
        for var in ds.coords:
            if var != "time":
                ds[var].attrs = variable_attributes[var]
                encoding[var] = dict(_FillValue=None)  # remove the default _FillValue attribute from each variable

        # save file
        ds.to_netcdf(nc_filepath, encoding=encoding)
        log.info(f"Saved {nc_filepath}")
